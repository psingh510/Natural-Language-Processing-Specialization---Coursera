{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda0e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trax\n",
      "  Downloading trax-1.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting absl-py (from trax)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting funcsigs (from trax)\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting gin-config (from trax)\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gym (from trax)\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jax (from trax)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from trax)\n",
      "  Downloading jaxlib-0.4.30-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from trax) (3.7.1)\n",
      "Requirement already satisfied: numpy in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from trax) (1.24.3)\n",
      "Requirement already satisfied: psutil in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from trax) (5.9.0)\n",
      "Requirement already satisfied: scipy in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from trax) (1.10.1)\n",
      "Requirement already satisfied: six in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from trax) (1.16.0)\n",
      "Collecting tensorflow-datasets (from trax)\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting tensorflow-text (from trax)\n",
      "  Downloading tensorflow_text-2.17.0rc0-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from gym->trax) (2.2.1)\n",
      "Collecting gym-notices>=0.0.4 (from gym->trax)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->trax)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from jax->trax)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from matplotlib->trax) (2.8.2)\n",
      "Requirement already satisfied: click in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (8.0.4)\n",
      "Collecting dm-tree (from tensorflow-datasets->trax)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets->trax)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting promise (from tensorflow-datasets->trax)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (4.25.1)\n",
      "Requirement already satisfied: pyarrow in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (2.31.0)\n",
      "Collecting simple-parsing (from tensorflow-datasets->trax)\n",
      "  Downloading simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->trax)\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting termcolor (from tensorflow-datasets->trax)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: toml in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow-datasets->trax) (1.14.1)\n",
      "Collecting etils>=1.9.1 (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->trax)\n",
      "  Downloading etils-1.9.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting tensorflow<2.17,>=2.16.1 (from tensorflow-text->trax)\n",
      "  Downloading tensorflow-2.16.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->trax) (2024.6.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->trax)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->trax) (4.7.1)\n",
      "Requirement already satisfied: zipp in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->trax) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2023.7.22)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (23.5.26)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->trax)\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: setuptools in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (68.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading grpcio-1.65.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow-datasets->trax)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets->trax)\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets->trax)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (0.38.4)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->trax)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/pallavisingh/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->trax) (0.1.0)\n",
      "Downloading trax-1.4.1-py2.py3-none-any.whl (637 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.9/637.9 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.30-cp311-cp311-macosx_11_0_arm64.whl (66.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_text-2.17.0rc0-cp311-cp311-macosx_11_0_arm64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.9.2-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading tensorflow-2.16.2-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl (110 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.0-cp311-cp311-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, promise\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827624 sha256=cb830c33948931b5f651a8c791b9bc0ee5aaba3fb63d754d21a1b56042ce57ab\n",
      "  Stored in directory: /Users/pallavisingh/Library/Caches/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=63223778e5e53e35504b773f1a6ae62afd1413f8577454c69a04eee63b62ec94\n",
      "  Stored in directory: /Users/pallavisingh/Library/Caches/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built gym promise\n",
      "Installing collected packages: namex, libclang, gym-notices, gin-config, funcsigs, dm-tree, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, promise, optree, opt-einsum, ml-dtypes, importlib_resources, immutabledict, h5py, gym, grpcio, google-pasta, gast, etils, docstring-parser, astunparse, absl-py, tensorboard, simple-parsing, rich, jaxlib, googleapis-common-protos, tensorflow-metadata, keras, jax, tensorflow, tensorflow-text, tensorflow-datasets, trax\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.1\n",
      "    Uninstalling protobuf-4.25.1:\n",
      "      Successfully uninstalled protobuf-4.25.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 dm-tree-0.1.8 docstring-parser-0.16 etils-1.9.2 funcsigs-1.0.2 gast-0.6.0 gin-config-0.5.0 google-pasta-0.2.0 googleapis-common-protos-1.63.2 grpcio-1.65.0 gym-0.26.2 gym-notices-0.0.8 h5py-3.11.0 immutabledict-4.2.0 importlib_resources-6.4.0 jax-0.4.30 jaxlib-0.4.30 keras-3.4.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 promise-2.3 protobuf-4.25.3 rich-13.7.1 simple-parsing-0.1.5 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 tensorflow-datasets-4.9.6 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.15.0 tensorflow-text-2.17.0rc0 termcolor-2.4.0 trax-1.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878c3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random as rnd\n",
    "\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874f623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/pallavisingh/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pallavisingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import Layer, load_tweets, process_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6777b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5., dtype=float32, weak_type=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.array(5.0)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff3baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4ab7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76a66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0 is 125.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bd3d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function\n",
    "grad_f = trax.fastmath.grad(fun=f)  # df / dx - Gradient of function f(x) with respect to x\n",
    "\n",
    "# View the type of the retuned object (it's a function)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f0d5ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(75., dtype=float32, weak_type=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the newly created function and pass in a value for x (the DeviceArray stored in 'a')\n",
    "grad_calculation = grad_f(a)\n",
    "\n",
    "# View the result of calling the grad_f function\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043c51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split():\n",
    "    # Load positive and negative tweets\n",
    "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "    # View the total number of positive and negative tweets.\n",
    "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "    # Split positive set into validation and training\n",
    "    val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "    train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "    # Split negative set into validation and training\n",
    "    val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "    train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "    \n",
    "    # Combine training data into one set\n",
    "    train_x = train_pos + train_neg \n",
    "\n",
    "    # Combine validation data into one set\n",
    "    val_x  = val_pos + val_neg\n",
    "\n",
    "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
    "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "\n",
    "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51979e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93df175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(train_x):\n",
    "\n",
    "    # Include special tokens \n",
    "    # started with pad, end of line and unk tokens\n",
    "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    # Note that we build vocab using training data\n",
    "    for tweet in train_x: \n",
    "        processed_tweet = process_tweet(tweet)\n",
    "        for word in processed_tweet:\n",
    "            if word not in Vocab: \n",
    "                Vocab[word] = len(Vocab)\n",
    "    \n",
    "    return Vocab\n",
    "\n",
    "Vocab = get_vocab(train_x)\n",
    "\n",
    "print(\"Total words in vocab are\",len(Vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69c14",
   "metadata": {},
   "source": [
    "# Converting a tweet to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddc23b",
   "metadata": {},
   "source": [
    "Instructions: Write a program tweet_to_tensor that takes in a tweet and converts it to an array of numbers. You can use the Vocab dictionary you just found to help create the tensor.\n",
    "\n",
    "Use the vocab_dict parameter and not a global variable.\n",
    "Do not hard code the integer value for the __UNK__ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3173be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token = '__UNK__', verbose = False):\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = [] \n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.        \n",
    "        word_ID = vocab_dict.get(word,unk_ID)\n",
    "            \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2304515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " @heyclaireee is back! thnx God!!! i'm so happy :)\n",
      "\n",
      "Tensor of tweet:\n",
      " [443, 2, 303, 566, 56, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[1])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[1], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8175056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
