{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q037OOv6pgXb"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "1. Load and preprocess data\n",
    "    - Load and tokenize data.\n",
    "    - Split the sentences into train and test sets.\n",
    "    - Replace words with a low frequency by an unknown marker `<unk>`.\n",
    "\n",
    "2. Develop N-gram based language models\n",
    "    - Compute the count of n-grams from a given data set.\n",
    "    - Estimate the conditional probability of a next word with k-smoothing.\n",
    "    \n",
    "3. Evaluate the N-gram models by computing the perplexity score.\n",
    "4. Use your own model to suggest an upcoming word given your sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MHw__5vphEV",
    "outputId": "18b3b192-2c49-4cbc-cd6e-e0bb2b701266"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pallavisingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YyaskGZpnvn"
   },
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "XiMdifAHpjbr",
    "outputId": "529079d5-6e63-4109-e290-e5bdb72a5b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open('./data/en_US.twitter.txt','r') as f:\n",
    "  data = f.read()\n",
    "\n",
    "\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHsHaI0ltlwV"
   },
   "source": [
    "Preprocess this data with the following steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter.\n",
    "2. Split each sentence into tokens. 3. Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
    "4. Assign sentences into train or test sets.\n",
    "5. Find tokens that appear at least N times in the training data.\n",
    "6. Replace tokens that appear less than N times by <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "um7P5Otnty76"
   },
   "outputs": [],
   "source": [
    "def split_sentences(data):\n",
    "\n",
    "  sentences = data.split('\\n')\n",
    "\n",
    "  sentences = [i.strip() for i in sentences]\n",
    "  sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yfVqUN42ugc0"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "  tokens_lists = []\n",
    "  for s in sentences:\n",
    "\n",
    "    tokenized = nltk.word_tokenize(s.lower())\n",
    "    tokens_lists.append(tokenized)\n",
    "\n",
    "  return tokens_lists\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upAss7BHvWOd",
    "outputId": "e00b8b70-340e-4f78-c2c5-f07fee0c32b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'a', 'girl'], ['you', 'are', 'a', 'boy']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentences(['I am a girl','you are a boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TQptJrNgvf1J"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "\n",
    "    # Get the sentences by splitting up the data\n",
    "    sentences = split_sentences(data)\n",
    "\n",
    "    # Get the list of lists of tokens by tokenizing the sentences\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njzrjk6YwH7P",
    "outputId": "b69b42e5-2a90-48fa-a9cb-ac50cb6a1833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your function\n",
    "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R6HNuM9GwMRW"
   },
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(53)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data)*0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liL92wdsw4AN",
    "outputId": "c8a5175c-89c0-49ce-fefa-5778ec64caad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample:\n",
      "['singing', 'instead', 'of', 'doing', 'my', 'project', '.']\n",
      "First test sample\n",
      "['right', 'back', 'at', 'you', 'from', 'scottsdale', '!', '\\\\m/']\n"
     ]
    }
   ],
   "source": [
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "\n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TmpvJjSBxCRt"
   },
   "outputs": [],
   "source": [
    " def count_words(tokenize_sentences):\n",
    "  word_freq = {}\n",
    "\n",
    "  for s in tokenize_sentences:\n",
    "\n",
    "    for t in s:\n",
    "\n",
    "      if t in word_freq.keys():\n",
    "        word_freq[t] += 1\n",
    "\n",
    "      else:\n",
    "        word_freq[t] = 1\n",
    "\n",
    "  return word_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD5InMCHymgd",
    "outputId": "dbc88598-deb6-4689-9990-53bfa5d33a5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 2,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(get_tokenized_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn-Yt_F3y_lL"
   },
   "source": [
    "### Handling 'Out of Vocabulary' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2THbboUXyuxu"
   },
   "outputs": [],
   "source": [
    "def get_words_higher_freq(tokenize_sentences,count_threshold):\n",
    "  count_dict= count_words(tokenize_sentences)\n",
    "  closed_vocab = []\n",
    "  for i in count_dict.keys():\n",
    "\n",
    "    if count_dict[i] >= count_threshold:\n",
    "      closed_vocab.append(i)\n",
    "\n",
    "  return closed_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wk5JqT840EGr",
    "outputId": "5907808a-d358-42fd-c025-a49c6478ad81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_higher_freq(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KtkijZDc0GEI"
   },
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    updated_tokenized_sentences = []\n",
    "    vocabulary_set = set(vocabulary)  # Convert vocabulary to a set for faster look-up\n",
    "\n",
    "    for s in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for t in s:\n",
    "            if t in vocabulary_set:\n",
    "                replaced_sentence.append(t)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        updated_tokenized_sentences.append(replaced_sentence)\n",
    "\n",
    "    return updated_tokenized_sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kr3d-4JR1FzG",
    "outputId": "62c0322e-e464-497a-dbe5-4e7c7ca4d188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', '<unk>', 'blue', '<unk>'],\n",
       " ['leaves', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['are', 'are', 'red', '.']]\n",
    "vocabulary = ['leaves', 'blue']\n",
    "replace_oov_words_by_unk(tokenized_sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kEolWy6-2TQj"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold, unknown_token ='unk'):\n",
    "\n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_higher_freq(train_data,count_threshold)\n",
    "\n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary,unknown_token=unknown_token)\n",
    "\n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary,unknown_token=unknown_token)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return train_data_replaced, test_data_replaced, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-IUC0Tw4V7o",
    "outputId": "424a9e8e-4e8e-49db-e08d-07d08a3a4981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['unk', 'are', 'unk', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train,\n",
    "                                                           tmp_test,\n",
    "                                                           count_threshold = 1\n",
    "                                                          )\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "36A9mkJP4WWB"
   },
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data,\n",
    "                                                                        test_data,\n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYocM9wf4jWh"
   },
   "source": [
    "### Develop n-gram based language models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YrwmAACP4hi9"
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = '<s>' , end_token = '<e>'):\n",
    "  n_grams = {}\n",
    "\n",
    "  for s in data:\n",
    "    s =  [start_token] * n + s + [end_token]\n",
    "\n",
    "    s_tup = tuple(s)\n",
    "\n",
    "    for i in range(len(s_tup)) if n == 1 else range(len(s_tup) - n + 1):\n",
    "\n",
    "      n_gram = s_tup[i:i+n]\n",
    "\n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "\n",
    "  return n_grams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZQKnh3z_hR8",
    "outputId": "7e19f4f2-4a37-4562-fa9e-e2c00da29d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MCQ8YJqM_jdV"
   },
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram,\n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    count_previous_n_gram = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "\n",
    "    denominator = count_previous_n_gram + vocabulary_size *k\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "\n",
    "    numerator =  n_plus1_gram_count + k\n",
    "\n",
    "    probability = numerator/denominator\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PbcV7WGClOb",
    "outputId": "c695f23c-9e58-4a87-d650-ac4beee569d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uypuE5UiDGmY"
   },
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    count_previous_n_gram = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    vocabulary = vocabulary +[end_token, unknown_token]\n",
    "    denominator = count_previous_n_gram + len(vocabulary) *k\n",
    "    out_dict = {}\n",
    "    for word in vocabulary:\n",
    "      numerator = 0\n",
    "      n_plus1_gram = previous_n_gram + (word,)\n",
    "      n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "\n",
    "      numerator =  n_plus1_gram_count + k\n",
    "\n",
    "      probability = numerator/denominator\n",
    "      out_dict[word] = probability\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "469uTmeoG3sp",
    "outputId": "e6f57ba1-3182-4727-83a3-a20127945470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'i': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "krSoZE6DG6Me"
   },
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "\n",
    "  vocabulary = vocabulary + ['<e>','<unk>']\n",
    "\n",
    "  n_grams = []\n",
    "\n",
    "  for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]  # Correct slicing to get the n-gram\n",
    "        n_grams.append(n_gram)\n",
    "\n",
    "  n_grams = list(set(n_grams))\n",
    "\n",
    "  row_index = {n_gram : i for i, n_gram in enumerate(n_grams)}\n",
    "\n",
    "  col_index = {word : j for j , word in enumerate(vocabulary)}\n",
    "  nrow = len(n_grams)\n",
    "  ncol = len(vocabulary)\n",
    "\n",
    "  count_matrix = np.zeros((nrow,ncol))\n",
    "\n",
    "  for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "\n",
    "    n_gram = n_plus1_gram[0:-1]\n",
    "    word = n_plus1_gram[-1]\n",
    "\n",
    "    if word not in vocabulary:\n",
    "            continue\n",
    "    i = row_index[n_gram]\n",
    "    j = col_index[word]\n",
    "    count_matrix[i, j] = count\n",
    "\n",
    "\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "c3lVjRhzJ3Pt",
    "outputId": "074b594d-a8fb-46d3-9b3d-0257783ab6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         like  this  dog    i    a   is  cat  <e>  <unk>\n",
       "(like,)   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(this,)   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(dog,)    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(is,)     0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(i,)      0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(<s>,)    0.0   0.0  0.0  1.0  0.0  0.0  0.0  0.0    0.0\n",
       "(cat,)    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(a,)      0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DHoThcnjJ_J_"
   },
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "Z50GXiD9KlBm",
    "outputId": "d6e8ca53-4138-4d87-a9c6-d6cf58a12b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             like      this       dog         i         a        is       cat  \\\n",
       "(like,)  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(this,)  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(dog,)   0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(is,)    0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(i,)     0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(<s>,)   0.100000  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(cat,)   0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "(a,)     0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(like,)  0.111111  0.111111  \n",
       "(this,)  0.111111  0.111111  \n",
       "(dog,)   0.111111  0.111111  \n",
       "(is,)    0.111111  0.111111  \n",
       "(i,)     0.111111  0.111111  \n",
       "(<s>,)   0.100000  0.100000  \n",
       "(cat,)   0.111111  0.111111  \n",
       "(a,)     0.111111  0.111111  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWums4XhKonI"
   },
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "H6aeSTJUKmy6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_perplexity(test_data, n_gram_counts, vocabulary, n):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of an n-gram model on test data.\n",
    "\n",
    "    Args:\n",
    "    test_data (list of list of str): Tokenized test sentences.\n",
    "    n_gram_counts (dict): Counts of n-grams.\n",
    "    vocabulary (list of str): List of unique words in the vocabulary.\n",
    "    n (int): The n in n-gram.\n",
    "\n",
    "    Returns:\n",
    "    float: Perplexity score.\n",
    "    \"\"\"\n",
    "    N = 0  # Total number of n-grams in the test set\n",
    "    log_likelihood = 0.0  # Sum of log probabilities\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    vocabulary_set = set(vocabulary)\n",
    "\n",
    "    for sentence in test_data:\n",
    "        # Pad the sentence with start tokens\n",
    "        sentence = ['<s>'] * (n - 1) + sentence\n",
    "\n",
    "        # Loop through the sentence and calculate the log-likelihood\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = tuple(sentence[i:i + n])\n",
    "            n_minus_1_gram = tuple(sentence[i:i + n - 1])\n",
    "            next_word = sentence[i + n - 1]\n",
    "\n",
    "            # Calculate n-gram and (n-1)-gram counts\n",
    "            n_gram_count = n_gram_counts.get(n_gram, 0)\n",
    "            n_minus_1_gram_count = sum(\n",
    "                count for ngram, count in n_gram_counts.items() if ngram[:-1] == n_minus_1_gram)\n",
    "\n",
    "            # Calculate the probability with add-one smoothing\n",
    "            probability = (n_gram_count + 1) / (n_minus_1_gram_count + vocabulary_size)\n",
    "\n",
    "            # Update log-likelihood\n",
    "            log_likelihood += np.log(probability)\n",
    "            N += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = np.exp(-log_likelihood / N)\n",
    "    return perplexity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJnPpvYFLTVv",
    "outputId": "21aaa0aa-5e87-492c-f557-ed1d918775c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4.445419733444931\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "n_gram_counts = {\n",
    "    ('<s>', 'I', 'love'): 2,\n",
    "    ('I', 'love', 'Python'): 2,\n",
    "    ('love', 'Python', 'is'): 1,\n",
    "    ('Python', 'is', 'great'): 3,\n",
    "    ('is', 'great', '</s>'): 2\n",
    "}\n",
    "vocabulary = ['<s>', 'I', 'love', 'Python', 'is', 'great', '</s>']\n",
    "test_data = [[\"I\", \"love\", \"Python\"], [\"Python\", \"is\", \"great\"]]\n",
    "n = 3\n",
    "\n",
    "perplexity = calculate_perplexity(test_data, n_gram_counts, vocabulary, n)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYwsaHa1MLMe"
   },
   "source": [
    "# Build an auto-complete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "z0iwNlNTLf5S"
   },
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary,end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "\n",
    "  n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "  previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "  probabilities = estimate_probabilities(previous_n_gram,n_gram_counts,n_plus1_gram_counts, vocabulary, k=k)\n",
    "\n",
    "  suggestion = None\n",
    "  max_prob = 0\n",
    "\n",
    "  for word, prob in probabilities.items():\n",
    "\n",
    "    if start_with != None:\n",
    "      if not word.startswith(start_with):\n",
    "\n",
    "        continue\n",
    "\n",
    "    if prob > max_prob:\n",
    "\n",
    "      suggestion = word\n",
    "\n",
    "      max_prob = prob\n",
    "\n",
    "  return suggestion, max_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_1lqJ1kSGai",
    "outputId": "080cd9a3-a5b5-484a-94d8-98cde0486fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-E8C-kMSL-I",
    "outputId": "41a7add2-2c87-4902-acb8-7ec5b8d7df40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfYNGSa-TiK2"
   },
   "source": [
    "The function defined below loop over varioud n-gram models to get multiple suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dZRvt7vSTjhz"
   },
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "  model_counts = len(n_gram_counts_list)\n",
    "  suggestions = []\n",
    "\n",
    "  for i in range(model_counts-1):\n",
    "    n_gram_counts =  n_gram_counts_list[i]\n",
    "\n",
    "    n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "\n",
    "    suggestion = suggest_a_word(previous_tokens,n_gram_counts,n_plus1_gram_counts,vocabulary,k = 1, start_with=start_with)\n",
    "\n",
    "    suggestions.append(suggestion)\n",
    "\n",
    "  return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJN5RnQzUrqN",
    "outputId": "8782cb0c-5cb8-49c5-8535-a4950335d8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('code', 0.08333333333333333), ('code', 0.09090909090909091)]\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = [\n",
    "    {('I',): 10, ('love',): 5, ('to',): 7},  # Unigram counts\n",
    "    {('I', 'love'): 4, ('love', 'to'): 3},   # Bigram counts\n",
    "    {('I', 'love', 'to'): 2}                 # Trigram counts\n",
    "]\n",
    "vocabulary = ['I', 'love', 'to', 'code', 'Python']\n",
    "previous_tokens = ['I', 'love']\n",
    "\n",
    "suggestions = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with='c')\n",
    "print(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d-892PIUsfb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
